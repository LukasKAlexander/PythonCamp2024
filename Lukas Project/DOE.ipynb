{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "648995e9-0587-4847-91c4-f97a5b944044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Webscaping DOE Press Releases\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4daeeb0-19d0-44bb-8362-3f65f4d451ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0963bbc-75f8-437c-91fc-ebc27edfd067",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80afaa3c-6fa0-4f47-9644-1e161e4c0b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#DOE from Biden\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Open CSV file\n",
    "with open('DOEpressreleasesBiden.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Set up Selenium driver\n",
    "    driver_path = Service('/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver')\n",
    "    driver = webdriver.Chrome(service=driver_path)\n",
    "    \n",
    "    # Loop through each page of the DOE press releases\n",
    "    for page_num in range(0, 61): \n",
    "        url = f'https://www.ed.gov/news/press-releases?page={page_num}'\n",
    "        driver.get(url)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)  # Wait for the page to load\n",
    "        \n",
    "        # Parse the page HTML with BeautifulSoup\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Find and process each press release on the page\n",
    "        releases = soup.find_all('li', class_='views-row')\n",
    "\n",
    "        for release in releases:  # Loop through each press release item\n",
    "            try:\n",
    "                fac = {}  # Empty dict to store the press release data\n",
    "                \n",
    "                # Extract Date\n",
    "                date_elem = release.find('span', class_='field-content date-display-single')\n",
    "                fac['Date'] = date_elem.text.strip() if date_elem else ''\n",
    "                \n",
    "                # Extract URL\n",
    "                url_elem = release.find('a')\n",
    "                if url_elem and 'href' in url_elem.attrs:\n",
    "                    fac['URL'] = f\"https://www.ed.gov{url_elem['href']}\"\n",
    "                else:\n",
    "                    fac['URL'] = ''\n",
    "                \n",
    "                # Extract Title\n",
    "                title_elem = release.find('h2', class_='field-content')\n",
    "                fac['Title'] = title_elem.text.strip() if title_elem else ''\n",
    "\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser', from_encoding='utf-8')\n",
    "\n",
    "                # Extract Text from the <div> with class 'field-name-body'\n",
    "                body_elem = interior_soup.find('div', class_='field field-name-body field-type-text-with-summary field-label-hidden')\n",
    "                if body_elem:\n",
    "                    text_divs = body_elem.find_all('div', class_='field-item even')\n",
    "                    raw_text = \"\\n\".join(div.get_text(separator='\\n').strip() for div in text_divs)\n",
    "                    fac['Text'] = clean_text(raw_text)  # Clean the extracted text\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "\n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing release: {e}\")\n",
    "                continue  # Skip to the next item in case of an error\n",
    "\n",
    "    # Close the driver after all pages have been processed\n",
    "    driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "685ff220-c350-47ac-8fe0-cd16a1002b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing release: Message: timeout: Timed out receiving message from renderer: 296.727\n",
      "  (Session info: chrome=127.0.6533.120)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010f90b8b8 chromedriver + 5179576\n",
      "1   chromedriver                        0x000000010f9032ea chromedriver + 5145322\n",
      "2   chromedriver                        0x000000010f47a2b0 chromedriver + 389808\n",
      "3   chromedriver                        0x000000010f462afa chromedriver + 293626\n",
      "4   chromedriver                        0x000000010f46281a chromedriver + 292890\n",
      "5   chromedriver                        0x000000010f460d1b chromedriver + 285979\n",
      "6   chromedriver                        0x000000010f4611df chromedriver + 287199\n",
      "7   chromedriver                        0x000000010f47022b chromedriver + 348715\n",
      "8   chromedriver                        0x000000010f48650d chromedriver + 439565\n",
      "9   chromedriver                        0x000000010f48b8eb chromedriver + 461035\n",
      "10  chromedriver                        0x000000010f461867 chromedriver + 288871\n",
      "11  chromedriver                        0x000000010f48636f chromedriver + 439151\n",
      "12  chromedriver                        0x000000010f505dfe chromedriver + 962046\n",
      "13  chromedriver                        0x000000010f4e8553 chromedriver + 841043\n",
      "14  chromedriver                        0x000000010f4b97f6 chromedriver + 649206\n",
      "15  chromedriver                        0x000000010f4ba05e chromedriver + 651358\n",
      "16  chromedriver                        0x000000010f8ceb20 chromedriver + 4930336\n",
      "17  chromedriver                        0x000000010f8d3a36 chromedriver + 4950582\n",
      "18  chromedriver                        0x000000010f8d4105 chromedriver + 4952325\n",
      "19  chromedriver                        0x000000010f8b0ee9 chromedriver + 4808425\n",
      "20  chromedriver                        0x000000010f8d43f9 chromedriver + 4953081\n",
      "21  chromedriver                        0x000000010f8a2844 chromedriver + 4749380\n",
      "22  chromedriver                        0x000000010f8f35c8 chromedriver + 5080520\n",
      "23  chromedriver                        0x000000010f8f3787 chromedriver + 5080967\n",
      "24  chromedriver                        0x000000010f902ece chromedriver + 5144270\n",
      "25  libsystem_pthread.dylib             0x00007ff8128b418b _pthread_start + 99\n",
      "26  libsystem_pthread.dylib             0x00007ff8128afae3 thread_start + 15\n",
      "\n",
      "Error processing release: Message: timeout: Timed out receiving message from renderer: 298.750\n",
      "  (Session info: chrome=127.0.6533.120)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010f90b8b8 chromedriver + 5179576\n",
      "1   chromedriver                        0x000000010f9032ea chromedriver + 5145322\n",
      "2   chromedriver                        0x000000010f47a2b0 chromedriver + 389808\n",
      "3   chromedriver                        0x000000010f462afa chromedriver + 293626\n",
      "4   chromedriver                        0x000000010f46281a chromedriver + 292890\n",
      "5   chromedriver                        0x000000010f460d1b chromedriver + 285979\n",
      "6   chromedriver                        0x000000010f4611df chromedriver + 287199\n",
      "7   chromedriver                        0x000000010f47022b chromedriver + 348715\n",
      "8   chromedriver                        0x000000010f48650d chromedriver + 439565\n",
      "9   chromedriver                        0x000000010f48b8eb chromedriver + 461035\n",
      "10  chromedriver                        0x000000010f461867 chromedriver + 288871\n",
      "11  chromedriver                        0x000000010f48636f chromedriver + 439151\n",
      "12  chromedriver                        0x000000010f505dfe chromedriver + 962046\n",
      "13  chromedriver                        0x000000010f4e8553 chromedriver + 841043\n",
      "14  chromedriver                        0x000000010f4b97f6 chromedriver + 649206\n",
      "15  chromedriver                        0x000000010f4ba05e chromedriver + 651358\n",
      "16  chromedriver                        0x000000010f8ceb20 chromedriver + 4930336\n",
      "17  chromedriver                        0x000000010f8d3a36 chromedriver + 4950582\n",
      "18  chromedriver                        0x000000010f8d4105 chromedriver + 4952325\n",
      "19  chromedriver                        0x000000010f8b0ee9 chromedriver + 4808425\n",
      "20  chromedriver                        0x000000010f8d43f9 chromedriver + 4953081\n",
      "21  chromedriver                        0x000000010f8a2844 chromedriver + 4749380\n",
      "22  chromedriver                        0x000000010f8f35c8 chromedriver + 5080520\n",
      "23  chromedriver                        0x000000010f8f3787 chromedriver + 5080967\n",
      "24  chromedriver                        0x000000010f902ece chromedriver + 5144270\n",
      "25  libsystem_pthread.dylib             0x00007ff8128b418b _pthread_start + 99\n",
      "26  libsystem_pthread.dylib             0x00007ff8128afae3 thread_start + 15\n",
      "\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#DOE from Trump admin (via wayback machine)\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Open CSV file\n",
    "with open('DOEpressreleasesTrump.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Set up Selenium driver\n",
    "    driver_path = Service('/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver')\n",
    "    driver = webdriver.Chrome(service=driver_path)\n",
    "    \n",
    "    # Loop through each page of the DOE press releases\n",
    "    for page_num in range(1, 44): \n",
    "        url = f'https://web.archive.org/web/20210108025343/https://www.ed.gov/news/press-releases?page={page_num}'\n",
    "        driver.get(url)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)  # Wait for the page to load\n",
    "        \n",
    "        # Parse the page HTML with BeautifulSoup\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Find and process each press release on the page\n",
    "        releases = soup.find_all('li', class_='views-row')\n",
    "\n",
    "        for release in releases:  # Loop through each press release item\n",
    "            try:\n",
    "                fac = {}  # Empty dict to store the press release data\n",
    "                \n",
    "                # Extract Date\n",
    "                date_elem = release.find('span', class_='field-content date-display-single')\n",
    "                fac['Date'] = date_elem.text.strip() if date_elem else ''\n",
    "                \n",
    "                # Extract URL\n",
    "                url_elem = release.find('a')\n",
    "                if url_elem and 'href' in url_elem.attrs:\n",
    "                    fac['URL'] = f\"https://web.archive.org/web/20210108025343/{url_elem['href']}\"\n",
    "                else:\n",
    "                    fac['URL'] = ''\n",
    "                \n",
    "                # Extract Title\n",
    "                title_elem = release.find('h2', class_='field-content')\n",
    "                fac['Title'] = title_elem.text.strip() if title_elem else ''\n",
    "\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser', from_encoding='utf-8')\n",
    "\n",
    "                # Extract Text from the <div> with class 'field-name-body'\n",
    "                body_elem = interior_soup.find('div', class_='field field-name-body field-type-text-with-summary field-label-hidden')\n",
    "                if body_elem:\n",
    "                    text_divs = body_elem.find_all('div', class_='field-item even')\n",
    "                    raw_text = \"\\n\".join(div.get_text(separator='\\n').strip() for div in text_divs)\n",
    "                    fac['Text'] = clean_text(raw_text)  # Clean the extracted text\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "\n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing release: {e}\")\n",
    "                continue  # Skip to the next item in case of an error\n",
    "\n",
    "    # Close the driver after all pages have been processed\n",
    "    driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
