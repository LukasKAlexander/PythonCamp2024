{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f14851b8-5de2-4cdd-97d5-9f8b9c981e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Webscaping USDA Press Releases\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67d62ddf-0171-4356-b6da-1b75ed73efd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11458366-5b23-49da-8295-be2dda9c6d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00eafe69-4bb5-4edc-89d3-83b6bc1fc482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#THIS SECTION DOES ARCHIVE (prior to 2021 Biden admin)\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Open CSV file\n",
    "with open('USDApressreleasesarchive.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Set up Selenium driver\n",
    "    driver_path = Service('/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver')\n",
    "    driver = webdriver.Chrome(service=driver_path)\n",
    "    \n",
    "    # Loop through each page of the USDA press releases\n",
    "    for page_num in range(0, 247): \n",
    "        url = f'https://www.usda.gov/media/press-releases/archive?page={page_num}'\n",
    "        driver.get(url)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)  # Wait for the page to load\n",
    "        \n",
    "        # Parse the page HTML with BeautifulSoup\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Find and process each press release on the page\n",
    "        cards = soup.find_all('li', {'class': 'news-releases-item'})\n",
    "\n",
    "        for card in cards:  # Loop through each news release item\n",
    "            try:\n",
    "                fac = {}  # Empty dict to store the press release data\n",
    "                \n",
    "                # Extract Date from the div with class 'news-release-date'\n",
    "                fac['Date'] = card.find('div', {'class': 'news-release-date'}).text.strip()\n",
    "                \n",
    "                # Extract URL from the <a> tag within the card\n",
    "                fac['URL'] = card.find('a')['href']\n",
    "                full_url = f\"https://www.usda.gov{fac['URL']}\"  # Construct full URL\n",
    "                \n",
    "                # Extract Title from the <a> tag\n",
    "                fac['Title'] = card.find('a').text.strip()\n",
    "                \n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(full_url)\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser', from_encoding='utf-8')\n",
    "\n",
    "                # Extract Text after the <aside> element\n",
    "                aside = interior_soup.find('aside', {'class': 'news-release-info'})\n",
    "                if aside:\n",
    "                    # Get the content after the <aside> element\n",
    "                    text_div = aside.find_next_sibling('div')\n",
    "                    fac['Text'] = text_div.get_text(separator='\\n').strip() if text_div else ''\n",
    "                else:\n",
    "                    fac['Text'] = ''  # If <aside> is not found\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing card: {e}\")\n",
    "                continue  # Skip to the next item in case of an error\n",
    "\n",
    "    # Close the driver after all pages have been processed\n",
    "    driver.quit()\n",
    "\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5efb883-2a93-46ea-804b-a5bbcb07f5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#This section collects for the Biden administration\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Open CSV file\n",
    "with open('USDApressreleasescurrentadminTEST.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Set up Selenium driver\n",
    "    driver_path = Service('/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver')\n",
    "    driver = webdriver.Chrome(service=driver_path)\n",
    "    \n",
    "    # Loop through each page of the USDA press releases\n",
    "    for page_num in range(0, 95): \n",
    "        url = f'https://www.usda.gov/media/press-releases?page={page_num}'\n",
    "        driver.get(url)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)  # Wait for the page to load\n",
    "        \n",
    "        # Parse the page HTML with BeautifulSoup\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Find and process each press release on the page\n",
    "        cards = soup.find_all('li', {'class': 'news-releases-item'})\n",
    "\n",
    "        for card in cards:  # Loop through each news release item\n",
    "            try:\n",
    "                fac = {}  # Empty dict to store the press release data\n",
    "                \n",
    "                # Extract Date from the div with class 'news-release-date'\n",
    "                fac['Date'] = card.find('div', {'class': 'news-release-date'}).text.strip()\n",
    "                \n",
    "                # Extract URL from the <a> tag within the card\n",
    "                fac['URL'] = card.find('a')['href']\n",
    "                full_url = f\"https://www.usda.gov{fac['URL']}\"  # Construct full URL\n",
    "                \n",
    "                # Extract Title from the <a> tag\n",
    "                fac['Title'] = card.find('a').text.strip()\n",
    "                \n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(full_url)\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser', from_encoding='utf-8')\n",
    "\n",
    "                # Extract Text after the <aside> element\n",
    "                aside = interior_soup.find('aside', {'class': 'news-release-info'})\n",
    "                if aside:\n",
    "                    # Get the content after the <aside> element\n",
    "                    text_div = aside.find_next_sibling('div')\n",
    "                    fac['Text'] = text_div.get_text(separator='\\n').strip() if text_div else ''\n",
    "                else:\n",
    "                    fac['Text'] = ''  # If <aside> is not found\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing card: {e}\")\n",
    "                continue  # Skip to the next item in case of an error\n",
    "\n",
    "    # Close the driver after all pages have been processed\n",
    "    driver.quit()\n",
    "\n",
    "print(\"All done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
