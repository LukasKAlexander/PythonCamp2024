{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81830875-552a-43e1-9f91-c2d6a050c537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Webscaping HUD Press Releases\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f203dcf0-7230-4d9a-9038-44699c7acb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764af07a-e4a8-4f7d-9919-e1dbc6839998",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ec2ad6-31ff-484c-b950-642b2cb7d4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#HUD 2024 DONE\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2024.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://www.hud.gov/press/press_releases_media_advisories'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('p')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://www.hud.gov{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with class 'field--name-field-press-body'\n",
    "                body_elem = interior_soup.find('div', class_='pr')\n",
    "                if body_elem:\n",
    "    # Extract both paragraphs and list items within <div class=\"pr\">\n",
    "                    paragraphs = body_elem.find_all(['p', 'li'])\n",
    "    # Clean and join the text from paragraphs and list items\n",
    "                    text_content = \"\\n\".join([clean_text(elem.get_text()) for elem in paragraphs])\n",
    "                    fac['Text'] = clean_text(text_content)  # Clean the extracted text\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f40d79-76d1-4ccb-8b38-92dcc9c2e864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "Error processing release: 'NoneType' object is not callable\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#HUD 2023 DONE\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2023.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://www.hud.gov/press/press_releases_media_advisories/2023'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('p')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://www.hud.gov{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with class 'field--name-field-press-body'\n",
    "                body_elem = interior_soup.find('div', class_='pr')\n",
    "                if body_elem:\n",
    "    # Extract both paragraphs and list items within <div class=\"pr\">\n",
    "                    paragraphs = body_elem.find_all(['p', 'li'])\n",
    "    # Clean and join the text from paragraphs and list items\n",
    "                    text_content = \"\\n\".join([clean_text(elem.get_text()) for elem in paragraphs])\n",
    "                    fac['Text'] = clean_text(text_content)  # Clean the extracted text\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632ef22a-d36f-499b-94f0-baa68935e2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#HUD 2022 DONE\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2022.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2022.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93af9b83-34a8-40fb-bc97-74554bd70a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#HUD 2021 DONE\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2021.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2021.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "980a3c21-b155-4a75-8bba-646e48329dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#HUD 2020 DONE\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2020.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2020.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d51143c4-01d5-498f-92a1-7b534ef14724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#HUD 2019 DONE\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2019.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2019.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a8ad52-ee3b-447d-a939-e544bd61c1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#HUD 2018 ready to go!\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2018.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2018.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a324aa6f-ac35-47fe-a104-938c6f3956c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#HUD 2017 DONE\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2017.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2017.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e23a896f-f9a9-48da-abb2-d0eb1c22d1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#HUD 2016 DONE\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2016.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2016.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c296eb9-016b-4c73-904a-01bae62a07f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#HUD 2015 DONE\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2015.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2015.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe8a9cc0-b59e-430e-bc0a-fb30b59c9817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#HUD 2014 DONE\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2014.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2014.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "368fb945-fd44-4baa-8197-ebfe34d12f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#HUD 2013 DONE\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2013.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2013.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "816b932e-464a-43bf-b207-1ba397c9d606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#HUD 2012 ready to go!\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2012.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2012.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d249496-720e-4086-a3fe-29b9cc53899c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#HUD 2011 DONE\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2011.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2011.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92eb9d9-011b-407a-aea1-de25dd01b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HUD 2010 ready to go!\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2010.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2010.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b471e-630f-4f91-9f43-a61d92500eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HUD 2009 ready to go!\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2009.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2009.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98e251e-43a7-4d8f-9834-b344e0579a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HUD 2008 ready to go!\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2008.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2008.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b2b3cb-bc80-425c-b2e5-c0e57c1ac4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HUD 2007 ready to go!\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2007.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2007.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd979ea9-8c33-44bb-8a79-8a778e0e782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HUD 2006 ready to go!\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2006.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2006.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01947111-397d-4d1b-81c9-6e89fcc834cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HUD 2005 ready to go!\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2005.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2005.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a8bc6-b221-43e1-bce6-d035a9b21533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HUD 2004 ready to go!\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2004.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2004.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37d7802-436b-4330-ba90-3bfc4296e54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HUD 2003 ready to go!\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2003.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2003.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f203379a-7916-4b1a-99b6-91b92ce76ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HUD 2002 ready to go!\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2002.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2002.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca92175-ec23-4dc5-b87a-1aee9fe0c80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HUD 2001 ready to go!\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2001.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2001.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd582d5-af5b-4141-a116-9d5ca3e39979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HUD 2000 ready to go!\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases2000.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/2000.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4610f4-3940-4f15-bd04-1a4f86a1c75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HUD 1999 ready to go!\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases1999.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/1999.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337643dd-5e8b-4690-90f7-2fe0cb465bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HUD 1998 ready to go!\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases1998.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/1998.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7e544a-875b-442c-ba7f-7c2d04c0e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HUD 1997 ready to go!\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to clean text by replacing multiple spaces with a single space and removing excessive newlines\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver_path = '/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Lukas Project/chromedriver'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# CSV formation\n",
    "with open('HUDpressreleases1997.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Date\", \"URL\", \"Title\", \"Text\"])\n",
    "    w.writeheader()\n",
    "\n",
    "    # Open the HUD press releases page\n",
    "    url = 'https://archives.hud.gov/news/1997.cfm'\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll to load more content if needed (adjust scroll strategy as necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Adjust this sleep time as needed to allow content to load\n",
    "    \n",
    "    # Parse the page with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find and process each press release on the page\n",
    "    releases = soup.find_all('li')  # Adjusted selector based on the shared HTML structure\n",
    "\n",
    "    for release in releases:\n",
    "        try:\n",
    "            fac = {}  # Empty dict to store the press release data\n",
    "            \n",
    "            # Extract Date from the <p> tag\n",
    "            date_text = release.contents[0] if release.contents else ''\n",
    "            fac['Date'] = date_text.strip() if date_text else ''\n",
    "            \n",
    "            # Extract URL and Title from the <a> tag inside the <p>\n",
    "            link = release.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                fac['URL'] = f\"https://archives.hud.gov/{link['href'].strip()}\"\n",
    "                fac['Title'] = link.text.strip()  # Title is the text within the <a> tag\n",
    "            else:\n",
    "                fac['URL'] = ''\n",
    "                fac['Title'] = ''\n",
    "\n",
    "            # Only proceed if URL is valid\n",
    "            if fac['URL']:\n",
    "                # Open the interior page using Selenium\n",
    "                driver.get(fac['URL'])\n",
    "                time.sleep(3)  # Allow time for page load\n",
    "                \n",
    "                # Scrape the interior page\n",
    "                interior_html = driver.page_source\n",
    "                interior_soup = BeautifulSoup(interior_html, 'html.parser')\n",
    "\n",
    "                # Extract Text from the <div> with id 'main'\n",
    "                div_elem = interior_soup.find('div', id='main')\n",
    "\n",
    "                # Initialize an empty list to store the text\n",
    "                text_content = []\n",
    "                \n",
    "                if div_elem:\n",
    "                    # Extract relevant content such as headers, paragraphs, and list items\n",
    "                    elements = div_elem.find_all(['p', 'ul', 'li'])\n",
    "\n",
    "                    # Flag to determine if we should stop processing\n",
    "                    stop_processing = False\n",
    "\n",
    "                    for elem in elements:\n",
    "                        if stop_processing:\n",
    "                            break\n",
    "                        \n",
    "                        # Check if the current element is the <p align=\"center\">###</p> tag\n",
    "                        if elem.name == 'p' and elem.get('align') == 'center' and elem.get_text(strip=True) == '###':\n",
    "                            stop_processing = True\n",
    "                            continue  # Skip adding this tag to the content\n",
    "\n",
    "                        if elem.name == 'li':\n",
    "                            # Handle list items\n",
    "                            clean_elem_text = f\"• {clean_text(elem.get_text())}\"\n",
    "                        else:\n",
    "                            clean_elem_text = clean_text(elem.get_text())\n",
    "                        \n",
    "                        text_content.append(clean_elem_text)\n",
    "\n",
    "                    # Join the cleaned text into a single string, separated by newlines\n",
    "                    fac['Text'] = \"\\n\".join(text_content)\n",
    "                else:\n",
    "                    fac['Text'] = ''\n",
    "                \n",
    "                # Write the data to CSV\n",
    "                w.writerow(fac)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing release: {e}\")\n",
    "            continue  # Skip to the next item in case of an error\n",
    "\n",
    "# Close the driver after all pages have been processed\n",
    "driver.quit()\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
