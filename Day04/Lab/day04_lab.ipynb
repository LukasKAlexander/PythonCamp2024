{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Web Scraping + File I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instructions: \n",
    "\n",
    "1. Go to https://polisci.wustl.edu/people/88/all OR https://polisci.wustl.edu/people/list/88/all\n",
    "2. Go to the page for each of the professors.\n",
    "3. Create a `.csv`` file with the following information for each professor:\n",
    "\t- Name\n",
    "\t- Title\n",
    "\t- E-mail\n",
    "\t- Web page\n",
    "\t- Specialization  \n",
    "\t\t- If they do not have a specialization, you can leave it blank. \n",
    "\t\t- An example from Deniz's page: https://polisci.wustl.edu/people/deniz-aksoy\n",
    "\t\t- Professor Aksoyâ€™s research is motivated by an interest in comparative political institutions and political violence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m web_page \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(web_address) \u001b[38;5;66;03m# open the web page\u001b[39;00m\n\u001b[1;32m     27\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(web_page\u001b[38;5;241m.\u001b[39mread()) \u001b[38;5;66;03m# soup the web page\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m dept_div \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdept\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n\u001b[1;32m     29\u001b[0m all_profs \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# find the list of names and parties\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m): \u001b[38;5;66;03m# for members 1 and 2 (member 0 is just the table heading)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m   \u001b[38;5;66;03m# you should also add try/except language to ensure a weird item doesn't break your whole scraper\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages/bs4/element.py:2433\u001b[0m, in \u001b[0;36mResultSet.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   2432\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   2434\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResultSet object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. You\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m key\n\u001b[1;32m   2435\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: ResultSet object has no attribute 'text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "driver_path = Service('/Users/ea025/Desktop/Python Camp - Alexander/PythonCamp2024/Day04/Lecture/chromedriver')  # Update with your path\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Run in headless mode if needed\n",
    "driver = webdriver.Chrome(service=driver_path, options=options)\n",
    "\n",
    "\n",
    "with open('professors.csv', 'w') as f: # set up with the writer\n",
    "  w = csv.DictWriter(f, fieldnames = (\"name\", \"title\", \"email\", \"Web page\", \"Specialization\")) # define column names\n",
    "  w.writeheader() # write the header\n",
    "  web_address='https://polisci.wustl.edu/people/list/88/all' # the web address\n",
    "  prof_address='https://polisci.wustl.edu/people/'\n",
    "\n",
    "\n",
    "  web_page = urllib.request.urlopen(web_address) # open the web page\n",
    "  soup = BeautifulSoup(web_page.read()) # soup the web page\n",
    "  dept_div = soup.find_all('div', class_='dept').text\n",
    "  all_profs = soup.find_all('a') # find the list of names and parties\n",
    "  for i in range(2,-3): # for members 1 and 2 (member 0 is just the table heading)\n",
    "    # you should also add try/except language to ensure a weird item doesn't break your whole scraper\n",
    "    try:\n",
    "      member = {} ## empty dictionary to fill in\n",
    "      member_i = all_members[i].find_all('a') # subset lower to each individual item\n",
    "      member[\"name\"] = names = [i.text in all_headings] # member's name\n",
    "      member['title'] = titles = [i.text for i in dept_div] # member's party\n",
    "      inner_page_url = web_address + member_i[0].a['href'] # get the extension to their personal page\n",
    "      inner_page = urllib.request.urlopen(inner_page_url) # open the personal page\n",
    "      inner_soup = BeautifulSoup(inner_page.read()) # soup the personal page\n",
    "      member['email'] = inner_soup.find_all('a', href=lambda href: href and href.startswith('mailto:')).text # get phone number\n",
    "      website['website'] = inner_page\n",
    "      Specialization['Specialization'] = interests_div.find_all('li')\n",
    "    except:\n",
    "      member['name'] = 'NA'\n",
    "      member['title'] = 'NA'\n",
    "      member['email'] = 'NA'\n",
    "    w.writerow(member) # write the row for this specific member\n",
    "    time.sleep(random.uniform(1, 5)) # be polite, sleep!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
